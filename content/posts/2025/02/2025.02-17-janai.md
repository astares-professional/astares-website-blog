---
title: "Run your AI system (LLM) locally"
date: 2025-02-17T09:00:00-07:00
lastmod: 2025-02-17T09:00:00-07:00
draft: false
categories: ["AI"]
tags: ["AI","Architecture", "LLM"]
description: "How to run a LLM locally"
slug: "local-ai"
---

# Running AI locally

*Artificial Intelligence has evolved rapidly, transforming industries and redefining how we interact with technology. While many AI services run in the cloud, there’s growing interest in running AI models locally.*

*Why is this a trend worth considering? And how can you do it effectively? Let’s dive in.*

## Why Run AI Locally?
Running AI locally offers several key advantages, the two primary ones are:

### Privacy & Security

    When AI models operate on your device, you’re not sending sensitive data to external servers. This is crucial for industries where data security and compliance matter, such as healthcare or finance.

### Cost Efficiency

    Cloud-based AI often comes with recurring costs—compute fees, storage costs, API usage, etc. Running models locally reduces dependence on cloud providers and can save money in the long run.

With local AI models one can eliminate network delays, with the right computation power provide also faster responses in applications that might require real-time processing. An AI developer can also fine-tune models, control updates, and optimize a custom model without being bound by third-party services.

## Hugging Face 

If you work with AI then there is one page that you should visit: Hugging Face ([https://huggingface.co/](https://huggingface.co/)) which is a leading AI platform. It is providing services to the open community as it makes AI more accessible - it serves as a **hub for open-source AI models, datasets, and tools**.

So it works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine Learning. 

If you work in Python you can install a special Hugging Face command line interface (CLI):

```
python -m pip install huggingface_hub
huggingface-cli login
```
to interact easily with the platform.

Similar to GitHub where you provide code repositories, Hugging Face allows you to create **AI model repositories**. There is free storage space for such *public repositories* and they charge only for the *private repositories*.

But HuggingFace is primarily a webpage - it is nothing that runs locally. But it is important because this is the source where you get the models from. To run them you need a local engine:

## Llama.cpp

[Llama.cpp](https://github.com/ggml-org/llama.cpp) is an open-source C/C++ framework designed for running large language models (LLMs) efficiently on local devices.
It was implemented in 2023 and optimized for running LLMs on CPUs and GPUs with minimal dependencies. So it is fast and **lightweight solution for running Large language models** such as [LLaMA](https://en.wikipedia.org/wiki/Llama_(language_model)).

The official repository with all the code is on 
https://github.com/ggml-org/llama.cpp

**Llama.cpp** also comes with small tools like a *[llama-cli](https://github.com/ggml-org/llama.cpp/blob/master/tools/main/main.cpp)* to give a prompt text on the command line if you want to call it from your program.

## Jan.ai

But compiling and running might be to cumbersome for most people. One easy way to get a local AI is therefore to install the **Jan.ai** software and platform which you can download from

- [https://jan.ai](https://jan.ai/)

Once installed tthis software allows you to easily download models and interact with them through a chat prompt. **As anything is running locally nothing from the context or question is shared with a third party.**

![Jan.ai Desktop](../images/jan.ai.png)

The [default engine of Jan.ai is Llama.cpp](https://jan.ai/docs/local-engines/llama-cpp). I have Jan.ai installed in a VirtualBox VM and it runs fast enough to give me proper answers in a similar response time as ChatGPT and other.

If you want to improve it the **source code** of Jan.ai is available at [https://github.com/menloresearch/jan](https://github.com/menloresearch/jan)

### Cortex
What is interesting is that Jan.ai underneath is powered by **Cortex** - which is a framework but also a command line tool again. 

- [https://cortex.so](https://cortex.so)

Compared to Llama.cpp it already comes in the form  of a DLL but also with a REST API. 

This gives you many options:
 - bind to Cortex with an FFI mechanism to use the Cortext library (DLL)
 - call Cortext using the REST API
 - call Cortex on the command line

The source code is available on [https://github.com/menloresearch/cortex.cpp](https://github.com/menloresearch/cortex.cpp)

So you can easily download Cortex for Linux:
```
curl -s https://raw.githubusercontent.com/menloresearch/cortex/main/engine/templates/linux/install.sh | sudo bash
```
and run
```
cortex start
```
to run the local server.

Using the [API Reference](https://cortex.so/api-reference/) from [https://cortex.so/api-reference/](https://cortex.so/api-reference/) it is easy to call the REST interface.


## llama3.java

If you develop in **Java** and need and interface to AI I doubt your favourite solution will be to bind to the mentioned engines locally. For sure one could bind to Cortex lib using JNI, but this requires some effort.

So you might better want to call the REST interface of Cortex to provide AI functionality.

But there is also another option: use an inference engine written in pure Java. Such a beast exists and the source code is available in 

- [https://github.com/mukel/llama3.java](https://github.com/mukel/llama3.java)

You can embed the file into your own project and provide a full self-contained solution.

If you want to go fast I recommend to check out [GraalVM](https://www.graalvm.org/) which gives you better Java performance due to AOT (Ahead of Time) compilation.

Check this [video with some more details](https://www.youtube.com/watch?v=VVUngUrMjAo&t=5356s) and a demo.

## Summary

There are several options to run a LLM locally without the necessity to share data and infos with a third party. The future for sure will have even more options.

It makes sense to run your own AI and stay fully in control. Combined with MCP as the universal adapter your local AI this will open up many new possibilities as MCP (Model Context Protocol) is as a standard way for AI applications and agents to connect to and work with your data sources. 

